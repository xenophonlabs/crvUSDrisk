{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Start of the Slippage Analysis\n",
    "\n",
    "# Empirical slippage analysis\n",
    "import csv\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# token0 is usdc, token1 is eth\n",
    "def process_uni_eth_trades(data1,name):\n",
    "    data1.rename(columns={\"amoun0_adjusted\":\"amount0_adjusted\"},inplace=True)\n",
    "    data1[\"amount1_adjusted\"]=data1[\"amount1_adjusted\"].apply(lambda x: abs(x/1.0e12))\n",
    "    data1[\"amount0_adjusted\"]=data1[\"amount0_adjusted\"].apply(lambda x: abs(x*1.0e12))\n",
    "    data1[\"price_implied\"] = 1e24/data1[\"price_implied\"]\n",
    "    data1[\"price_actual\"] = 1e24/data1[\"price_actual\"]\n",
    "    data1[\"previous_price_actual\"] = 1e24/data1[\"previous_price_actual\"]\n",
    "    data1[\"price_impact\"]=abs(data1[\"price_actual\"]-data1[\"previous_price_actual\"])/data1[\"previous_price_actual\"]\n",
    "    data1[\"evt_block_time\"] = pd.to_datetime(data1['evt_block_time'], utc=True)\n",
    "    data1['unix_timestamp'] = data1['evt_block_time'].astype(int) // 10**9\n",
    "    data1.sort_values(by=\"unix_timestamp\", axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last')\n",
    "    data1['date'] = pd.to_datetime(data1['unix_timestamp'], unit='s').dt.date\n",
    "    df = data1.groupby([\"date\"]).agg({\"price_impact\": \"std\",\"unix_timestamp\":[\"first\",\"last\"],\"price_actual\":\"mean\"}).rename(columns={\"price_impact\": \"volatility\"})    \n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.columns = ['_'.join(col).rstrip('_') for col in df_reset.columns.values]\n",
    "    df_reset['rolling_volatility'] = df_reset['volatility_std'].rolling(window=7).mean()*((252 ** 0.5))\n",
    "    # df_reset[\"std_price\"] = df_reset['price_actual_mean'].rolling(window=7).apply(lambda x: np.std(x))/df_reset['price_actual_mean'].rolling(window=7).mean()\n",
    "\n",
    "   # Then, perform the asof merge\n",
    "    merged_df = pd.merge_asof(data1, df_reset, left_on='unix_timestamp', right_on='unix_timestamp_first',direction='backward')\n",
    "    merged_df = merged_df.dropna()\n",
    "    merged_df[\"new_cross_impact\"]=merged_df[\"rolling_volatility\"]*merged_df[\"amount1_adjusted\"]\n",
    "    print(merged_df.columns,merged_df.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    # plt.plot(merged_df[\"date_x\"],merged_df[\"std_price\"]/merged_df[\"price_actual_mean\"], label='std_price')\n",
    "    plt.plot(merged_df[\"date_x\"],merged_df[\"rolling_volatility\"],label='rolling_volatility')\n",
    "    plt.scatter(merged_df[\"date_x\"],merged_df[\"price_impact\"],label='price_impact',alpha=0.1,c=merged_df[\"amount1_adjusted\"],cmap='viridis')\n",
    "    plt.legend()\n",
    "    plt.title(name)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Trade Size')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    # plt.plot(merged_df[\"date_x\"],merged_df[\"std_price\"]/merged_df[\"price_actual_mean\"], label='std_price')\n",
    "    plt.plot(merged_df[\"date_x\"],merged_df[\"rolling_volatility\"],label='rolling_volatility')\n",
    "    plt.scatter(merged_df[\"date_x\"],1e3*merged_df[\"price_impact\"]/merged_df[\"amount1_adjusted\"],label='price_impact/trade_size',alpha=0.1,c=merged_df[\"rolling_volatility\"],cmap='viridis')\n",
    "    plt.legend()\n",
    "    plt.title(name)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('rolling_volatility')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(merged_df[\"amount1_adjusted\"],merged_df[\"price_movement_percentage\"],c=merged_df[\"rolling_volatility\"],cmap='viridis')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(merged_df[\"unix_timestamp\"],merged_df[\"price_actual\"])    \n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    return(merged_df)\n",
    "\n",
    "# token0 is usdc, token1 is usdt\n",
    "def process_uni_stable_trades(data1, name):\n",
    "    data1.rename(columns={\"amoun0_adjusted\":\"amount0_adjusted\"},inplace=True)\n",
    "    data1.rename(columns={\"amoun1_adjusted\":\"amount0_adjusted\"},inplace=True)\n",
    "    data1[\"amount1_adjusted\"]=data1[\"amount1_adjusted\"].apply(lambda x: abs(x))\n",
    "    data1[\"amount0_adjusted\"]=data1[\"amount0_adjusted\"].apply(lambda x: abs(x))\n",
    "    data1[\"price_implied\"] = 1/data1[\"price_implied\"]\n",
    "    data1[\"price_actual\"] = 1/data1[\"price_actual\"]\n",
    "    data1[\"previous_price_actual\"] = 1/data1[\"previous_price_actual\"]\n",
    "    data1[\"price_impact\"]=abs(data1[\"price_actual\"]-data1[\"previous_price_actual\"])/data1[\"previous_price_actual\"]\n",
    "    data1[\"evt_block_time\"] = pd.to_datetime(data1['evt_block_time'], utc=True)\n",
    "    data1['unix_timestamp'] = data1['evt_block_time'].astype(int) // 10**9\n",
    "    data1.sort_values(by=\"unix_timestamp\", axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last')\n",
    "    data1['date'] = pd.to_datetime(data1['unix_timestamp'], unit='s').dt.date\n",
    "    df = data1.groupby([\"date\"]).agg({\"price_impact\": \"std\",\"unix_timestamp\":[\"first\",\"last\"],\"price_actual\":\"mean\"}).rename(columns={\"price_impact\": \"volatility\"})    \n",
    "    df_reset = df.reset_index()\n",
    "    df_reset.columns = ['_'.join(col).rstrip('_') for col in df_reset.columns.values]\n",
    "    df_reset['rolling_volatility'] = df_reset['volatility_std'].rolling(window=7).mean()*((252 ** 0.5))\n",
    "    # df_reset[\"std_price\"] = df_reset['price_actual_mean'].rolling(window=7).apply(lambda x: np.std(x))/df_reset['price_actual_mean'].rolling(window=7).mean()\n",
    "    \n",
    "    # Then, perform the asof merge\n",
    "    merged_df = pd.merge_asof(data1, df_reset, left_on='unix_timestamp', right_on='unix_timestamp_first',direction='backward')\n",
    "    merged_df = merged_df.dropna()\n",
    "    merged_df[\"new_cross_impact\"]=merged_df[\"rolling_volatility\"]*merged_df[\"amount1_adjusted\"]\n",
    "    print(merged_df.columns,merged_df.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    # plt.plot(merged_df[\"date_x\"],merged_df[\"std_price\"]/merged_df[\"price_actual_mean\"], label='std_price')\n",
    "    plt.plot(merged_df[\"date_x\"],merged_df[\"rolling_volatility\"],label='rolling_volatility')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(merged_df[\"amount1_adjusted\"],merged_df[\"price_movement_percentage\"],c=merged_df[\"rolling_volatility\"],cmap='viridis')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(merged_df[\"unix_timestamp\"],merged_df[\"price_actual\"])    \n",
    "    plt.show()\n",
    "    return(merged_df)\n",
    "\n",
    "def gen_multi_var_reg(df, x_vars, y_var):    \n",
    "    # Define input (X) and output (y) variables\n",
    "    X = df[x_vars]\n",
    "    y = df[y_var]\n",
    "\n",
    "    # Add a constant term for the intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # # Get the summary\n",
    "    # print(model.summary())\n",
    "\n",
    "    # # To get individual components (parameters, R-squared, etc.)\n",
    "    # print(\"Parameters:\", model.params)\n",
    "    # print(\"R-squared:\", model.rsquared)\n",
    "    return model\n",
    "\n",
    "def lin_reg(df, x_var=\"amount1_adjusted\", y_var=\"price_impact\",name=None,color_var=\"rolling_volatility\"):\n",
    "    print(\"lin_reg\")\n",
    "    # Linear regression\n",
    "    # Generate Y-values based on the regression model\n",
    "    p, cov = np.polyfit(df[x_var], df[y_var], 1, cov=True)  # Assuming x and y are your data arrays\n",
    "    slope = p[0]\n",
    "    intercept = p[1]\n",
    "    errors = np.sqrt(np.diag(cov))\n",
    "    y_fit = slope * df[x_var] + intercept\n",
    "    print(slope,intercept,errors)\n",
    "    plt.scatter(x=df[x_var], y=df[y_var], label='Data', c=df[color_var], cmap='viridis')\n",
    "    plt.plot(df[x_var], y_fit, label=f'Fit: y = {slope:.6f}x + {intercept:.6f}', color='firebrick')\n",
    "    if x_var==\"amount1_adjusted\":\n",
    "        plt.xlabel(\"Trade Size\")\n",
    "    else:\n",
    "        plt.xlabel(x_var)\n",
    "    plt.ylabel(y_var)\n",
    "    plt.legend()\n",
    "    plt.title(name)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(color_var)\n",
    "    plt.show()\n",
    "    return(p, cov)\n",
    "\n",
    "def plot_3d_relationships(df):\n",
    "    df['volatility_bin'] = (df['rolling_volatility']*10000).astype(\"int\")/10000\n",
    "    df['new_cross_impact_bin'] = df['new_cross_impact'].round()\n",
    "    df['trade_size_bin'] = df['amount1_adjusted'].round()\n",
    "    \n",
    "    df2=df.groupby([\"volatility_bin\",\"trade_size_bin\"]).agg({\"price_impact\": \"mean\"}).reset_index()\n",
    "    print(df2[[\"volatility_bin\",\"trade_size_bin\",\"price_impact\"]])    \n",
    "    fig = px.scatter(df2,x=\"trade_size_bin\",y=\"price_impact\",color=\"volatility_bin\")\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(type='log', title='trade_size_bin'),\n",
    "        yaxis=dict(type='log', title='price_impact')\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    df['volatility_bin'] = (df['rolling_volatility']*10000).astype(\"int\")/10000\n",
    "    df['trade_size_bin'] = df['amount1_adjusted'].round()\n",
    "    df2=df.groupby([\"volatility_bin\",\"trade_size_bin\"]).agg({\"price_impact\": \"mean\"}).reset_index()\n",
    "    print(df2[[\"volatility_bin\",\"trade_size_bin\",\"price_impact\"]])\n",
    "    fig = px.scatter_3d(df2,x=\"volatility_bin\",y=\"trade_size_bin\",z=\"price_impact\",color=\"price_impact\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/slippage/eth_usdc_0_05.csv\",\"r\") as infile:\n",
    "    data1 = pd.read_csv(infile)\n",
    "eth_usdc_0_05_df = process_uni_eth_trades(data1,\"eth_usdc_0_05\")\n",
    "\n",
    "with open(\"../data/slippage/eth_usdc_0_3.csv\",\"r\") as infile:\n",
    "    data3 = pd.read_csv(infile)\n",
    "eth_usdc_0_3_df = process_uni_eth_trades(data3,\"eth_usdc_0_3_df\")\n",
    "\n",
    "with open(\"../data/slippage/usdct_usdt_0_01.csv\",\"r\") as infile:\n",
    "    data2 = pd.read_csv(infile)\n",
    "usdct_usdt_0_01_df = process_uni_stable_trades(data2)\n",
    "\n",
    "with open(\"../data/slippage/dai_usdc_0_01.csv\",\"r\") as infile:\n",
    "    data4 = pd.read_csv(infile)\n",
    "dai_usdc_0_01_df = process_uni_stable_trades(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1 = gen_multi_var_reg(eth_usdc_0_05_df,x_vars=[\"rolling_volatility\",\"amount1_adjusted\"],y_var=[\"price_impact\"])\n",
    "intercept = ols1.params[0]  # This is the intercept (often denoted as 'b0')\n",
    "slopes = ols1.params[1:]    # These are the slopes (coefficients for the predictors, 'b1', 'b2', ...)\n",
    "print(\"rsquared\",ols1.rsquared)\n",
    "print(\"rsquared_adj\",ols1.rsquared_adj)\n",
    "print(\"intercept\",intercept)\n",
    "print(\"slopes\",slopes)\n",
    "\n",
    "p,cov = lin_reg(eth_usdc_0_05_df, x_var=\"amount1_adjusted\", y_var=\"price_impact\",name=None,color_var=\"rolling_volatility\")\n",
    "slope = p[0]\n",
    "intercept = p[1]\n",
    "errors = np.sqrt(np.diag(cov))\n",
    "y_fit = slope * eth_usdc_0_05_df[\"amount1_adjusted\"] + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2 = gen_multi_var_reg(eth_usdc_0_3_df,x_vars=[\"rolling_volatility\",\"amount1_adjusted\"],y_var=[\"price_impact\"])\n",
    "print(ols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols3 = gen_multi_var_reg(usdct_usdt_0_01_df,x_vars=[\"rolling_volatility\",\"amount1_adjusted\"],y_var=[\"price_impact\"])\n",
    "print(ols3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols4 = gen_multi_var_reg(dai_usdc_0_01_df,x_vars=[\"rolling_volatility\",\"amount1_adjusted\"],y_var=[\"price_impact\"])\n",
    "print(ols4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(eth_usdc_0_05_df, x_var=\"amount1_adjusted\", y_var=\"price_impact\",name=None,color_var=\"rolling_volatility\")\n",
    "lin_reg(eth_usdc_0_3_df, x_var=\"amount1_adjusted\", y_var=\"price_impact\",name=None,color_var=\"rolling_volatility\")\n",
    "\n",
    "lin_reg(usdct_usdt_0_01_df, x_var=\"amount1_adjusted\", y_var=\"price_impact\",name=None,color_var=\"rolling_volatility\")\n",
    "lin_reg(dai_usdc_0_01_df, x_var=\"amount1_adjusted\", y_var=\"price_impact\",name=None,color_var=\"rolling_volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg(eth_usdc_0_3_df, x_var=\"amount1_adjusted\", y_var=\"new_cross_impact\",name=None,color_var=\"rolling_volatility\")\n",
    "lin_reg(usdct_usdt_0_01_df, x_var=\"amount1_adjusted\", y_var=\"new_cross_impact\",name=None,color_var=\"rolling_volatility\")\n",
    "lin_reg(dai_usdc_0_01_df, x_var=\"amount1_adjusted\", y_var=\"new_cross_impact\",name=None,color_var=\"rolling_volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_3d_relationships(eth_usdc_0_05_df)\n",
    "plot_3d_relationships(eth_usdc_0_3_df)\n",
    "plot_3d_relationships(usdct_usdt_0_01_df)\n",
    "# plot_3d_relationships(dai_usdc_0_01_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "# new slippage function based on empirical analysis\n",
    "def quad_output(x):\n",
    "    \"\"\"\n",
    "    @notice calculate price impact when selling x or y to the open market.\n",
    "    Assuming that we trade against Uniswap is a conservative assumption\n",
    "    \"\"\"\n",
    "    a,b,c = (2.602424822058012e-10,2.3009044333523045e-07,0.0004613261845851614)\n",
    "    return(a*x**2+b*x+c)\n",
    "\n",
    "x = np.logspace(-9,5, num=50, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "y = [quad_output(x) for x in x]\n",
    "fig=px.line(x=x, y=y)\n",
    "fig.show()\n",
    "\n",
    "def lin_output(x):\n",
    "    return 1.081593506690093e-06*x+0.0004379110082802476\n",
    "\n",
    "x = np.logspace(-9,5, num=50, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "print(x)\n",
    "y = [lin_output(x) for x in x]\n",
    "fig=px.line(x=x, y=y)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
